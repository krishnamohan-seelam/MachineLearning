{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed by Yandex researchers and engineers, it is the successor of the MatrixNet algorithm that is widely used within the company for ranking tasks, forecasting and making recommendations. \n",
    "\n",
    "Gradient boosting is a powerful machine-learning technique that achieves state-of-the-art results\n",
    "in a variety of practical tasks. For a number of years, it has remained the primary method for\n",
    "learning problems with heterogeneous features, noisy data, and complex dependencies: web search,\n",
    "recommendation systems, weather forecasting, and many others\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Gradient Boosting Works ?\n",
    "Gradient boosting involves three elements:\n",
    "\n",
    "1. **A loss function to be optimized.**\n",
    "\n",
    "> - The loss function used depends on the type of problem being solved.\n",
    "> - It must be differentiable, but many standard loss functions are supported and you can define your own.\n",
    "For example, regression may use a squared error and classification may use logarithmic loss.\n",
    "> - A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used.\n",
    "\n",
    "\n",
    "2. **A weak learner to make predictions.**\n",
    "\n",
    "> - Decision trees are used as the weak learner in gradient boosting.\n",
    "> - Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and **“correct”** the residuals in the predictions.\n",
    "> - Trees are constructed in a greedy manner, choosing the best split points based on purity scores like Gini or to minimize the loss.\n",
    "> - It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes.\n",
    "This is to ensure that the learners remain weak, but can still be constructed in a greedy manner.\n",
    "\n",
    "\n",
    "3. **An additive model to add weak learners to minimize the loss function.**\n",
    "\n",
    "> - Trees are added one at a time, and existing trees in the model are not changed.\n",
    "> - A gradient descent procedure is used to minimize the loss when adding trees.\n",
    "> - Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error.\n",
    "> - Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by (reducing the residual loss.\n",
    "\n",
    "Generally this approach is called functional gradient descent or gradient descent with functions.\n",
    "\n",
    "The output for the new tree is then added to the output of the existing sequence of trees in an effort to correct or improve the final output of the model.\n",
    "\n",
    "A fixed number of trees are added or training stops once loss reaches an acceptable level or no longer improves on an external validation dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why CatBoost differs with others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost introduces two critical algorithmic advances \n",
    "- The implementation of ordered boosting, a permutation-driven alternative to the classic algorithm\n",
    "- An innovative algorithm for processing categorical features.\n",
    " \n",
    "> - In CatBoost, It generates s random permutations of our training dataset.Several permutations are then used to enhance the robustness of the algorithm \n",
    "> - Sample a random permutation are selected and obtain gradients on its basis. These are the same permutations as ones used for calculating statistics for categorical features. Different permutations for training distinct models, thus using several permutations does not lead to overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages of Catboost\n",
    "\n",
    "- **Good Quality** with default parameters\n",
    "- **Accurate**:Superior quality when compared with other GBDT libraries on many datasets.\n",
    "- **Robust**: reduces the need for extensive hyper-parameter tuning\n",
    "- **Easy to use**:Support for both numerical and categorical features.\n",
    "- **Works well** for small data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- [Introduction to GBDT](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "- http://learningsys.org/nips17/assets/papers/paper_11.pdf\n",
    "- [Anna Veronika Dorogush: Mastering gradient boosting with CatBoost | PyData London 2019](https://youtu.be/usdEWSDisS0)\n",
    "- https://github.com/catboost/tutorials/blob/master/classification/classification_tutorial.ipynb\n",
    "- https://github.com/catboost/tutorials/blob/master/events/2019_pydata_london/pydata_london_2019.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
