{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees \n",
    "#### **Reference  :hands-on-machine-learning  - Chapter 06**, few online blogs \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to train Decision Trees (also called “growing” trees). \n",
    "\n",
    "The algorithm works by first splitting the training set into two subsets using a single feature k and a threshold <sub>k</sub>.How does it choose k and tk? It searches for the pair (k, tk) that produces the purest subsets (weighted by their size).\n",
    "\n",
    "Below the cost function that the algorithm tries to minimize.\n",
    "\n",
    "### J(k,t<sub>k</sub>) = (m<sub>left</sub>/m) G<sub>left</sub> + (m<sub>right</sub>/m) G<sub>right</sub>\n",
    "\n",
    "Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity. \n",
    "\n",
    "A few other hyperparameters control additional stopping conditions (min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Impurity or Entropy?\n",
    "\n",
    "In Machine Learning, entropy is frequently used as an impurity measure: a set’s entropy is zero when it contains instances of only one class.\n",
    "\n",
    "### H<sub>i</sub>=−∑<sub>k=1 pi,k≠0</sub>p <sub>i,k</sub>log<sub>2</sub>(pi,k)\n",
    "\n",
    "should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. \n",
    "\n",
    "Gini impurity is slightly faster to compute, so it is a good default. \n",
    "\n",
    "However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees\n",
    "\n",
    "#### Rreduction of entropy is called Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Hyperparameters\n",
    "---\n",
    "\n",
    "Decision Trees make very few assumptions about the training data.If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely—indeed, most likely overfitting it. Such a model is often called a nonparametric model, not because it does not have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data.\n",
    "\n",
    "If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely—indeed, most likely overfitting it. Such a model is often called a nonparametric model, not because it does not have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data.\n",
    "\n",
    "In Scikit-Learn, this is controlled by the max_depth hyperparameter (the default value is None, which means unlimited). Reducing **max_depth** will regularize the model and thus reduce the risk of overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_depth** – the maximum depth of the tree\n",
    "\n",
    "**min_samples_split** - the minimum number of samples a node must have before it can be split\n",
    "\n",
    "**min_samples_leaf** - the minimum number of samples a leaf node must have\n",
    "\n",
    "**min_weight_fraction_leaf** - same as min_samples_leaf but expressed as a fraction of the total number of weighted instances\n",
    "\n",
    "**max_leaf_nodes** - the maximum number of leaf nodes\n",
    "\n",
    "**max_features** - maximum number of features that are evaluated for splitting at each node\n",
    "\n",
    "\n",
    "#### Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data. Random Forests can limit this instability by averaging predictions over many trees,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix:\n",
    "---\n",
    "\n",
    "| Actual/Prediction  |Positive   | Negative  |\n",
    "|--------------------|-----------|-----------|\n",
    "|Positive            |  TP       | FN        |\n",
    "|Negative            |  FP       | TN        |\n",
    "\n",
    "\n",
    "**True Positives (TP)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n",
    "\n",
    "**True Negatives (TN)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n",
    "\n",
    "False positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n",
    "\n",
    "**False Positives (FP)** – When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n",
    "\n",
    "It's also known as **Type 1 error** \n",
    "\n",
    "**False Negatives (FN)** – When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n",
    "\n",
    "It's also known as **Type 2 error** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics:\n",
    "---\n",
    "Once you understand these four parameters then we can calculate Accuracy, Precision, Recall and F1 score.\n",
    "\n",
    "**Accuracy** - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. For our model, we have got 0.803 which means our model is approx. 80% accurate.\n",
    "\n",
    "**Accuracy** = TP+TN/TP+FP+FN+TN\n",
    "\n",
    "**Precision** - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good.\n",
    "\n",
    "Used typically to avoid false positives\n",
    "\n",
    "**Precision** = TP/TP+FP\n",
    "\n",
    "**Recall (Sensitivity)** - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? We have got recall of 0.631 which is good for this model as it’s above 0.5.\n",
    "\n",
    "Used typically to avoid false negatives\n",
    "\n",
    "**Recall** = TP/TP+FN\n",
    "\n",
    "**F1 score** - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. In our case, F1 score is 0.701.\n",
    "\n",
    "**F1 Score** = 2*(Recall * Precision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Points to Note:\n",
    "- If the data contains noise or incomplete information, there is not a single machine learning technique that will generate a highly performant model.\n",
    "- Adding more features to the dataset can greatly improve the model's performance. Adding more models to an ensemble cannot improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
