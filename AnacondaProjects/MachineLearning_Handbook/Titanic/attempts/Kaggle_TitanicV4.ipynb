{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title- Titanic: Machine Learning from Disaster\n",
    "\n",
    "### Competition Description\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
    "\n",
    "#### Practice Skills\n",
    "###### Binary classification \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "The data has been split into two groups:\n",
    "\n",
    "training set (train.csv)\n",
    "test set (test.csv)\n",
    "\n",
    "The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n",
    "\n",
    "The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "\n",
    "We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n",
    "<tr>\n",
    "<td>survival</td>\n",
    "<td>Survival</td>\n",
    "<td>0 = No, 1 = Yes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>pclass</td>\n",
    "<td>Ticket class</td>\n",
    "<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>sex</td>\n",
    "<td>Sex</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Age</td>\n",
    "<td>Age in years</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>sibsp</td>\n",
    "<td># of siblings / spouses aboard the Titanic</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>parch</td>\n",
    "<td># of parents / children aboard the Titanic</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ticket</td>\n",
    "<td>Ticket number</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>fare</td>\n",
    "<td>Passenger fare</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>cabin</td>\n",
    "<td>Cabin number</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>embarked</td>\n",
    "<td>Port of Embarkation</td>\n",
    "<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Variable Notes\n",
    "<p><b>pclass</b>: A proxy for socio-economic status (SES)<br> 1st = Upper<br> 2nd = Middle<br> 3rd = Lower<br><br> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br><br> <b>sibsp</b>: The dataset defines family relations in this way...<br> Sibling = brother, sister, stepbrother, stepsister<br> Spouse = husband, wife (mistresses and fiancés were ignored)<br><br> <b>parch</b>: The dataset defines family relations in this way...<br> Parent = mother, father<br> Child = daughter, son, stepdaughter, stepson<br> Some children travelled only with a nanny, therefore parch=0 for them.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.Prepare Problem\n",
    "#### a.Load libraries\n",
    "#### b.Load dataset\n",
    "##### for this problem we will be loading training set and test  from two files as given by kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from mlsettings.settings import load_app_config, get_datafolder_path\n",
    "from mltools.mlcommon import load_data, print_dataset_info, split_dataset, auto_scatter_simple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "% matplotlib inline \n",
    "from numpy import set_printoptions\n",
    "set_printoptions(precision=4)\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_app_config()\n",
    "DIRECTORY=\"kaggle_titanic\"\n",
    "TRAIN_FILE ='train.csv'\n",
    "TEST_FILE = 'test.csv'\n",
    "RESPONSE = 'Survived'\n",
    "input_path = get_datafolder_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename=TRAIN_FILE,response=RESPONSE):\n",
    "    input_file = os.path.join(input_path, DIRECTORY, filename)\n",
    "    input_dataset = load_data(input_file)\n",
    "    print(\" input file is :{0} loaded.\".format(input_file))\n",
    "    #print(input_dataset.head())\n",
    "    \n",
    "    try:\n",
    "        continuous_vars = input_dataset.describe().columns.values.tolist()\n",
    "        print(\"Continous Variables\")\n",
    "        print(continuous_vars)\n",
    "    except ValueError:\n",
    "        print(\"No continous variables\")\n",
    "    \n",
    "    try:\n",
    "        categorical_vars = input_dataset.describe(include=[\"object\"]).columns.values.tolist()\n",
    "        print(\"Categorical Variables\")\n",
    "        print(categorical_vars)\n",
    "    except ValueError:\n",
    "        print(\"No categorical variables\")\n",
    "        categorical_vars = None\n",
    "    \n",
    "    response_column =  [col for col in input_dataset.columns if response in col]\n",
    "    feature_columns =  [col for col in input_dataset.columns if response not in col]\n",
    "      \n",
    "    return  input_dataset,feature_columns,response_column,continuous_vars,categorical_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset,feature_columns,response_column,continuous_vars,categorical_vars = load_dataset(filename=TRAIN_FILE,response=RESPONSE)\n",
    "train_X = train_dataset[feature_columns]\n",
    "train_y = train_dataset[response_column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset,tfeature_columns,tresponse_column,tcontinuous_vars,tcategorical_vars  = load_dataset(filename=TEST_FILE,response=RESPONSE)\n",
    "test_X =[]\n",
    "test_y=[]\n",
    "if feature_columns:\n",
    "    test_X = test_dataset[tfeature_columns]\n",
    "\n",
    "if response_column:\n",
    "    test_y = test_dataset[tfeature_columns]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from collections import Counter\n",
    "def detect_outliers(dataset,noutliers,columns):\n",
    "    outlier_indices = []\n",
    "    for column in columns:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(dataset[column], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(dataset[column],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = dataset[(dataset[column] < Q1 - outlier_step) | (dataset[column] > Q3 + outlier_step )].index\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "         \n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "     \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > noutliers )\n",
    "    return multiple_outliers \n",
    "        \n",
    "Outliers_to_drop = detect_outliers(train_dataset,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n",
    "print(train_dataset.loc[Outliers_to_drop])\n",
    "train_dataset = train_dataset.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n",
    "train_X = train_dataset[feature_columns]\n",
    "train_y = train_dataset[response_column]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_X.info())\n",
    "print(test_X.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Summarize Data \n",
    "#### a) Descriptive statistics\n",
    "#### b) Data visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_data_descriptives(input_dataset,X,feature_columns,y,response_column):\n",
    "    print(\"<{0} {1} {0}>\".format(\"=\"*40,\"info\"))\n",
    "    print(input_dataset.info())\n",
    "    print(\"<{0} {1} {0}>\".format(\"=\"*40,\"feature columns\"))\n",
    "    print(feature_columns)\n",
    "    print(\"<{0} {1} {0}>\".format(\"=\"*40,\"data header\"))\n",
    "    print(X.head().to_string())\n",
    "    print(\"<{0} {1} {0}>\".format(\"=\"*40,\"response\"))\n",
    "    print(response_column)\n",
    "    print(\"<{0} {1} {0}>\".format(\"=\"*40,\"Descriptive Statistics -X\"))\n",
    "    print(X.describe())\n",
    "    print(\"<{0} {1} {0}>\".format(\"=\"*40,\"Descriptive Statistics -y\"))\n",
    "    print(y.describe())\n",
    "    print(\"<{0} {1} {0}>\".format(\"=\"*40,\"value_count -y\"))\n",
    " \n",
    "    print(y.groupby(response_column)[response_column].count())\n",
    "    ##print(\"<{0} {1} {0}>\".format(\"=\"*40,\"Correlation\"))\n",
    "    ##print(input_dataset.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('precision', 4)\n",
    "display_data_descriptives(train_dataset,train_X,feature_columns,train_y,response_column)\n",
    "#display_data_descriptives(test_dataset,tfeature_columns,tresponse_column,tcontinuous_vars,tcategorical_vars)\n",
    "print(test_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = ['Sex', 'Embarked','SibSp','Parch','Pclass']\n",
    "def bar_plots(train_dataset,categorical):\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    size =len(categorical)\n",
    "     \n",
    "    for i in range(size):\n",
    "        #counts=train_dataset.groupby(categorical[i])['Survived'].value_counts()\n",
    "        #print(\"Dataset group by {0} \".format(categorical[i]))\n",
    "        #print(counts)\n",
    "        ax = fig.add_subplot(3, 2, i+1)\n",
    "        sns.barplot(x=categorical[i], y=\"Survived\", data=train_dataset,ax=ax,errwidth =0)\n",
    "        sns.despine()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "pkmn_type_colors = ['#78C850',  # Grass\n",
    "                    '#F08030',  # Fire\n",
    "                    '#6890F0',  # Water\n",
    "                    '#A8B820',  # Bug\n",
    "                    '#A8A878',  # Normal\n",
    "                    '#A040A0',  # Poison\n",
    "                    '#F8D030',  # Electric\n",
    "                    '#E0C068',  # Ground\n",
    "                    '#EE99AC',  # Fairy\n",
    "                    '#C03028',  # Fighting\n",
    "                    '#F85888',  # Psychic\n",
    "                    '#B8A038',  # Rock\n",
    "                    '#705898',  # Ghost\n",
    "                    '#98D8D8',  # Ice\n",
    "                    '#7038F8',  # Dragon\n",
    "                   ]\n",
    "\n",
    "bar_plots(train_dataset,categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences  from bar plots  \n",
    "##### Survival rate of female is more than males in all passenger categories\n",
    "##### Survival rate for passenger class 3 is least\n",
    "##### Survival rate of  passengers boarded at Embarked  at  C is greater than others\n",
    "##### Passengers havings siblings survived better  than  zero siblings\n",
    "##### Passengers travelling alone had less chances of survival than families \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g  = sns.factorplot(x=\"Pclass\", hue=\"Sex\", col=\"Survived\",data=train_dataset, kind=\"count\",size=5, aspect=.7,palette=flatui);\n",
    "#g1 = sns.factorplot(x=\"Embarked\", hue=\"Sex\", col=\"Survived\",data=train_dataset, kind=\"count\",size=5, aspect=.7,palette=flatui);   \n",
    "#g2 = sns.factorplot(x=\"SibSp\", col=\"Survived\",data=train_dataset, kind=\"count\",size=5, aspect=.7,palette=sns.color_palette(\"husl\",2)); \n",
    "#g3 = sns.factorplot(x=\"Parch\", col=\"Survived\",data=train_dataset, kind=\"count\",size=5, aspect=.7,palette=sns.color_palette(\"husl\",2)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='Sex',y='Survived',hue='Pclass',size=4, aspect=1,palette=flatui ,data =train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Women  from 1st  and 2nd class have 100 % survival\n",
    "##### Men from 2nd and 3rd Pclass have only around 10% survival chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g1 =sns.factorplot(x='Pclass', y='Survived', hue='Sex', col='Embarked', data=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Males from Pclass 1 only have slightly higher survival chance than Pclass 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "### 3. Prepare Data\n",
    "#### a) Data Cleaning\n",
    "#### b) Feature Selection\n",
    "#### c) Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_dataset = [train_dataset,test_dataset]\n",
    " \n",
    "## identity the null data sets \n",
    "for dataset in full_dataset:\n",
    "    print(\"<{0} {1} {0}>\".format(\"=\"*40,\"Columns having null values\"))\n",
    "    check_null = dataset.isnull().sum()[dataset.isnull().sum()>0] \n",
    "    print(check_null)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_char = lambda x : x[0]\n",
    "transform_cabin = lambda x : 1 if x!='X' else 0\n",
    "for dataset in full_dataset:\n",
    "    dataset['Cabin'].fillna('X' ,inplace=True)\n",
    "    dataset['Cabin']= dataset['Cabin'].map(first_char)\n",
    "    #dataset['Cabin']= dataset['Cabin'].map(transform_cabin)\n",
    "\n",
    "g = sns.factorplot(\"Survived\", col=\"Cabin\" ,col_wrap=4 ,data=full_dataset[0],kind=\"count\", size=3.5, aspect=.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in full_dataset:\n",
    "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.',expand=False)\n",
    "    \n",
    "print(train_dataset.groupby('Title')['Survived'].value_counts())\n",
    "print(test_dataset.groupby('Title')['Name'].count()) \n",
    "for dataset in full_dataset:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "full_dataset[0][['Title', 'Survived']].groupby(['Title'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ax  =sns.violinplot(x=\"Embarked\", y=\"Age\", hue=\"Survived\", data=train_dataset, split=True)\n",
    "fig = plt.figure(figsize=(8,4)) \n",
    "ax = sns.boxplot(y=\"Age\",x='Survived', data=train_dataset, palette=\"Set2\")\n",
    "ax.set_xticklabels(ax.get_xticklabels())\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(6,6)) \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "sns.barplot(x='Survived' , y='Age' ,hue ='Title',data=train_dataset,ax=ax,errwidth =0)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),ha='right')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mean_age =full_dataset[0][['Title', 'Age']].groupby(['Title'], as_index=False).mean().set_index('Title') \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "test_mean_age =full_dataset[1][['Title', 'Age']].groupby(['Title'], as_index=False).mean().set_index('Title') \n",
    "test_mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_dataset[0][full_dataset[0][\"Age\"].isnull()].groupby(['Title'], as_index=False)['Age'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_dataset[1][full_dataset[1][\"Age\"].isnull()].groupby(['Title'], as_index=False)['Age'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transform  sex  label in numerical categorical value, assign mean age  to null \n",
    "#####  Fill in missing Embarked values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_null_index =list(full_dataset[0][\"Age\"][full_dataset[0][\"Age\"].isnull()].index)\n",
    " \n",
    "for each_index in age_null_index:\n",
    "    title =full_dataset[0]['Title'].iloc[each_index]\n",
    "    if title =='Other':\n",
    "        full_dataset[0]['Age'].iloc[each_index] = -1\n",
    "    else:\n",
    "        age= train_mean_age.loc[title]['Age']\n",
    "        full_dataset[0]['Age'].iloc[each_index] = age\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_dataset[0][full_dataset[0][\"Age\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tage_null_index =list(full_dataset[1][\"Age\"][full_dataset[1][\"Age\"].isnull()].index)\n",
    " \n",
    "for each_index in tage_null_index:\n",
    "    title =full_dataset[1]['Title'].iloc[each_index]\n",
    "    if title =='Other':\n",
    "        full_dataset[1]['Age'].iloc[each_index] = -1\n",
    "    else:\n",
    "        age= test_mean_age.loc[title]['Age']\n",
    "        full_dataset[1]['Age'].iloc[each_index] = age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_dataset[1][full_dataset[1][\"Age\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sex_mapping= {'male':0,'female':1}\n",
    "for dataset in full_dataset:\n",
    "    #dataset['Sex'] =dataset['Sex'].map(sex_mapping)\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "    \n",
    "    '''\n",
    "    median_age = math.ceil(dataset[\"Age\"].median())\n",
    "    #dataset['Age'].fillna(median_age, inplace=True)\n",
    "    \n",
    "    age_null_index =list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n",
    "    print(len(age_null_index))\n",
    "    for each_index in age_null_index:\n",
    "        median_age = math.ceil(dataset[\"Age\"].median())\n",
    "        pred_age = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[each_index][\"SibSp\"]) &\n",
    "                                   (dataset['Parch'] == dataset.iloc[each_index][\"Parch\"]) &\n",
    "                                   (dataset['Pclass'] == dataset.iloc[each_index][\"Pclass\"]))].median()\n",
    "        if not np.isnan(pred_age) :\n",
    "            dataset['Age'].iloc[each_index] = pred_age\n",
    "        else :\n",
    "            dataset['Age'].iloc[each_index] = pred_age\n",
    "    '''\n",
    "    \n",
    "for dataset in full_dataset:\n",
    "    print(\"<{0} {1} {0}>\".format(\"=\"*40,\"Columns having null values\"))\n",
    "    check_null = dataset.isnull().sum()[dataset.isnull().sum()>0] \n",
    "    print(check_null)\n",
    "\n",
    "#test_dataset[test_dataset[\"Age\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transform Fare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_dataset['Age'].fillna(median_age, inplace=True)\n",
    "#full_dataset = [train_dataset,test_dataset]\n",
    "full_dataset[1][full_dataset[1][\"Fare\"].isnull()]\n",
    "\n",
    "print(full_dataset[1][  (full_dataset[1]['Pclass'] ==3  ) & \n",
    "                  (full_dataset[1]['Sex'] == 'male'  ) &\n",
    "                  (full_dataset[1]['Age'] >= 50  )\n",
    "               ])\n",
    "# assign same fare\n",
    "full_dataset[1]['Fare'].iloc[152]=14.5\n",
    "print(full_dataset[1][  (full_dataset[1]['Pclass'] ==3  ) & \n",
    "                  (full_dataset[1]['Sex'] == 'male'  ) &\n",
    "                  (full_dataset[1]['Age'] >= 50  )\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "def one_hot_dataframe(data,columns,replace=False):\n",
    "    fe_vec= feature_extraction.DictVectorizer()\n",
    "    make_dict = lambda row :dict((column,row[column]) for column in  columns)\n",
    "    vector_data=pd.DataFrame(fe_vec.fit_transform( data[columns].apply(make_dict, axis=1)).toarray())\n",
    "    vector_data.columns = fe_vec.get_feature_names()\n",
    "    vector_data.index= data.index\n",
    "    if replace:\n",
    "        data = data.drop(columns, axis=1)\n",
    "        data = data.join(vector_data)\n",
    "    return data,vector_data\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset,train_dataset_n = one_hot_dataframe(train_dataset, ['Pclass','Embarked', 'Sex','Title','Cabin'], replace=True)\n",
    "test_dataset,test_dataset_n = one_hot_dataframe(test_dataset, ['Pclass','Embarked', 'Sex','Title','Cabin'], replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_dataset = [train_dataset,test_dataset]\n",
    "train_dataset['AgeBand'] = pd.cut(train_dataset[train_dataset['Age']>-1]['Age']  ,5)\n",
    "train_dataset['AgeBand'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in full_dataset:\n",
    "    dataset.loc[ dataset['Age'] < 0, 'Age'] = -1\n",
    "    dataset.loc[ (dataset['Age'] > 0 ) & (dataset['Age'] <= 16.336), 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16.336) & (dataset['Age'] <= 32.252), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32.252) & (dataset['Age'] <= 48.168), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48.168) & (dataset['Age'] <= 64.084), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64.084, 'Age'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset['FareBand'] = pd.qcut(train_dataset['Fare'], 4)\n",
    "print (train_dataset[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean())\n",
    "\n",
    "for dataset in full_dataset:\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in full_dataset:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] +  dataset['Parch'] + 1\n",
    "    #dataset['IsAlone'] = 0\n",
    "    #dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    dataset['Single'] = dataset['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    dataset['SmallF'] = dataset['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\n",
    "    dataset['MedF'] = dataset['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "    dataset['LargeF'] = dataset['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_drop = ['Name', 'SibSp', 'Parch','FamilySize','Ticket']\n",
    "train_dataset = train_dataset.drop(features_drop, axis=1)\n",
    "test_dataset = test_dataset.drop(features_drop, axis=1)\n",
    "train_dataset = train_dataset.drop(['PassengerId', 'AgeBand', 'FareBand'], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### we will drop Cabin T\n",
    "X_train = train_dataset.drop(['Survived'], axis=1)\n",
    "y_train = train_dataset['Survived']\n",
    "X_test = test_dataset.drop(\"PassengerId\", axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train.columns.values)\n",
    "print(X_test.columns.values)\n",
    "all_features  =set(X_test.columns.values).intersection(set(X_train.columns.values))\n",
    "all_features =list(all_features)\n",
    "print(all_features)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train =X_train[all_features]\n",
    "X_test = X_test[all_features]\n",
    "X_train.shape, y_train.shape, X_test.shape\n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate Algorithms\n",
    "####  a) Split-out validation dataset\n",
    "####  b) Test options and evaluation metric\n",
    "####  c) Spot Check Algorithms\n",
    "####  d) Compare Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import  train_test_split\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "X_trainmodel, X_val, y_trainmodel, y_val = train_test_split(X_train, y_train, test_size=test_size,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logrmodel = LogisticRegression()\n",
    "logrmodel.fit(X_trainmodel, y_trainmodel.values.ravel())\n",
    "result = logrmodel.score(X_trainmodel, y_trainmodel.values)\n",
    "print (\"Accuracy: {0:.3f}\".format(result*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lin_svc = LinearSVC()\n",
    "lin_svc.fit(X_trainmodel, y_trainmodel.values.ravel())\n",
    "#y_pred_linear_svc = lin_svc.predict(X_test)\n",
    "acc_linear_svc = round(lin_svc.score(X_trainmodel, y_trainmodel) * 100, 2)\n",
    "print (acc_linear_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def train_and_evaluate(model, X_train, y_train, t_splits =10,seed=7):\n",
    "    model.fit(X_train, y_train)\n",
    "    print (\"Coefficient of determination on training set:\",model.score(X_train, y_train))\n",
    "    # create a k-fold cross validation iterator of k=5 folds\n",
    "    cv = KFold(n_splits= t_splits,shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv)\n",
    "    print(scores)\n",
    "    print (\"Average coefficient of determination using {0}-fold crossvalidation:{1}\".format(t_splits,np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "###models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('DT',DecisionTreeClassifier()))\n",
    "models.append(('KNN',KNeighborsClassifier()))\n",
    "models.append(('GB',GaussianNB()))\n",
    "models.append(('SVC',SVC()))\n",
    "models.append(('RFC',RandomForestClassifier(n_estimators=300,random_state=0,criterion='gini')))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, X_trainmodel, y_trainmodel.values.ravel(), cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"Accuracy of {0} is {1} with variance {2}\".format(name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def measure_performance(X, y, clf, show_accuracy=True,show_classification_report=True,\n",
    "                        show_confusion_matrix=True, show_r2_score=False):\n",
    "    y_pred = clf.predict(X) \n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format( metrics.accuracy_score(y, y_pred)) )\n",
    "    if show_classification_report:\n",
    "        print (\"Classification report\")\n",
    "        print (metrics.classification_report(y, y_pred))\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion matrix\") \n",
    "        print(metrics.confusion_matrix(y, y_pred),)\n",
    "    if show_r2_score:\n",
    "        print (\"Coefficient of determination:{0:.3f}\"\n",
    "               .format( metrics.r2_score(y, y_pred)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc =RandomForestClassifier(n_estimators=300,random_state=0 )\n",
    "print(rfc)\n",
    "rfc.fit(X_trainmodel, y_trainmodel.values.ravel())\n",
    "y_pred=measure_performance(X_val,y_val,rfc, show_accuracy=False, \n",
    "                    show_classification_report=True,\n",
    "                    show_confusion_matrix=True, show_r2_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "svc=SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "svc.fit(X_trscaled, y_trainmodel.values.ravel())\n",
    "y_pred=measure_performance(X_valscaled,y_val,svc, show_accuracy=False, \n",
    "                    show_classification_report=True,\n",
    "                    show_confusion_matrix=True, show_r2_score=False)\n",
    "                    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rfc= RandomForestClassifier()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "## Search grid for optimal parameters\n",
    "'''\n",
    "rf_param_grid = {\"max_depth\": [5,10],\n",
    "              \"max_features\": [3, 5, 10],\n",
    "              \"min_samples_split\": [10,20, 40],\n",
    "              \"min_samples_leaf\": [1, 3, 5],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,500],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "'''\n",
    "\n",
    "rf_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,500],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "grid_search = GridSearchCV(rfc,param_grid = rf_param_grid, cv=10, scoring=\"accuracy\", verbose = 1,n_jobs =-1)\n",
    "\n",
    "grid_search.fit(X_trainmodel,y_trainmodel.values.ravel())\n",
    "\n",
    "rfc_best = grid_search.best_estimator_\n",
    "\n",
    " #Best score\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_best_params =grid_search.best_params_\n",
    "#rfc =RandomForestClassifier(**rfc_best_params)\n",
    "rfc =RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=3,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.00001,\n",
    "            n_estimators=2000, n_jobs=1, oob_score=False, random_state=0,\n",
    "            verbose=0, warm_start=False)\n",
    "print(rfc)\n",
    "rfc.fit(X_trainmodel, y_trainmodel.values.ravel())\n",
    "y_pred=measure_performance(X_val,y_val,rfc, show_accuracy=False, \n",
    "                    show_classification_report=True,\n",
    "                    show_confusion_matrix=True, show_r2_score=False)\n",
    "\n",
    "#print(X_trainmodel.info())\n",
    "#print(X_test.info())\n",
    "#X_test.to_csv('tranform_test.csv', index=False)\n",
    "y_pred_result=rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(model,X_trainmodel):\n",
    "    features = pd.DataFrame()\n",
    "    features['feature'] = X_trainmodel.columns.values\n",
    "    features['importance'] = model.feature_importances_\n",
    "    features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "    features.set_index('feature', inplace=True)\n",
    "    fig = plt.figure(figsize=(8,6)) \n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    features.plot(kind='barh',ax=ax)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(features['importance'].nlargest(18).index)\n",
    "plot_feature_importances(rfc,X_trainmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features =['Sex=female', 'Title=Mr', 'Sex=male', 'Title=Miss', 'Cabin=X', 'Pclass', 'Title=Mrs', 'Fare', 'MedF', 'LargeF',\n",
    " 'Age', 'Embarked=S', 'Single', 'Title=Master', 'Embarked=C', 'Cabin=E', 'Cabin=B', 'Title=Other']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_trscaled = scaler.fit(X_trainmodel).transform(X_trainmodel)\n",
    "X_valscaled = scaler.fit_transform(X_val)\n",
    "svc=SVC()\n",
    "svc.fit(X_trscaled, y_trainmodel.values.ravel())\n",
    "y_pred=measure_performance(X_valscaled,y_val,svc, show_accuracy=False, \n",
    "                    show_classification_report=True,\n",
    "                    show_confusion_matrix=True, show_r2_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 'poly', 'rbf', 'sigmoid'\n",
    "param_grid = [{\"kernel\" : ['rbf'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "              {'kernel': ['linear'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
    "print(\"List of grids:\\n{}\".format(param_grid))\n",
    "grid_search = GridSearchCV(SVC(probability=True), param_grid, cv=5,n_jobs =-1)\n",
    "grid_search.fit(X_trscaled,y_trainmodel.values.ravel())\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_valscaled, y_val)))\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_best=grid_search.best_params_\n",
    "X_testscaled = scaler.fit_transform(X_test)\n",
    "svc=SVC(**svc_best)\n",
    "svc.fit(X_trscaled, y_trainmodel.values.ravel())\n",
    "y_pred=measure_performance(X_valscaled,y_val,svc, show_accuracy=False, \n",
    "                    show_classification_report=True,\n",
    "                    show_confusion_matrix=True, show_r2_score=False)\n",
    "y_pred_result=svc.predict(X_testscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "eclf = VotingClassifier(estimators=[('svc', svc), ('rf', rfc)], voting='hard')\n",
    "eclf = eclf.fit(X_trscaled, y_trainmodel.values.ravel())\n",
    "y_pred=measure_performance(X_valscaled,y_val,eclf, show_accuracy=False, \n",
    "                    show_classification_report=True,\n",
    "                    show_confusion_matrix=True, show_r2_score=False)\n",
    "y_pred_result=eclf.predict(X_testscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_X[\"PassengerId\"],\n",
    "        \"Survived\": y_pred_result\n",
    "    })\n",
    "submission.to_csv('submission_new_0209.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
